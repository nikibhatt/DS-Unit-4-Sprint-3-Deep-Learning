{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "\n",
    "with open(f'100-0.txt', 'r', errors='ignore') as f:\n",
    "    data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "# r = requests.get(url)\n",
    "# r.encoding = r.apparent_encoding\n",
    "# data = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "text = \" \".join(data)\n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 1122018\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "print('sequences:', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "    \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 12:12:58.810957 13032 deprecation.py:506] From C:\\Users\\test.NIDHNEMI\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1121920/1122018 [============================>.] - ETA: 0s - loss: 1.8174- ETA: 0s - loss: 1.\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"am glad to hear it.\n",
      "\n",
      "\n",
      "      VERGES.\n",
      "    \"\n",
      "am glad to hear it.\n",
      "\n",
      "\n",
      "      VERGES.\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"am glad to hear it.\n",
      "\n",
      "\n",
      "      VERGES.\n",
      "    \"\n",
      "am glad to hear it.\n",
      "\n",
      "\n",
      "      VERGES.\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"am glad to hear it.\n",
      "\n",
      "\n",
      "      VERGES.\n",
      "    \"\n",
      "am glad to hear it.\n",
      "\n",
      "\n",
      "      VERGES.\n",
      "                  O I have Hele fanter's with\n",
      "     you not shorge an what palfeat with hiinself\n",
      "    Icam not in of a shall rem'd never\n",
      "    Thates thereâ€™s with hepe wisines it wenter thy pronelfauct of rest!\n",
      "  VAENES. The oppace, ant best clurt af to him, me pore prerse,\n",
      "    So light word hight lowst upin\n",
      "He opfore of ofe.\n",
      "  BATRUCE. Ichers to thourâ€™s me?\n",
      "\n",
      "PAUST.\n",
      "GLandingard deseal of thy san.\n",
      "  Fo\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"am glad to hear it.\n",
      "\n",
      "\n",
      "      VERGES.\n",
      "    \"\n",
      "am glad to hear it.\n",
      "\n",
      "\n",
      "      VERGES.\n",
      "              Heathior Dustace! [Yy- iphill], visish mect;\n",
      "What, arfiam herâ€™sy infeam. H peas,\n",
      "    Why, a sindipsâ€™s foult sear heren infore,\n",
      "    For I when xerectizes earen nightdall by me. Mort!\n",
      "    Ccain duod, yet wittey, even him grage unallendyâ€™t?\n",
      "    Soranty! Whe thoue not into ent for the praceilank in;\n",
      "    But. Wot he; bling hor an wich it warn as mid;\n",
      "Hothly when he oâ€™at donotwer-tlees\n",
      "1122018/1122018 [==============================] - 886s 790us/sample - loss: 1.8174\n",
      "Epoch 2/2\n",
      "1121920/1122018 [============================>.] - ETA: 0s - loss: 1.7187\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"y part, Iâ€™ll not meddle nor\n",
      "make no fa\"\n",
      "y part, Iâ€™ll not meddle nor\n",
      "make no fair part the world the proverent to the world to the good the parter to the waster.\n",
      "\n",
      "PARTIAN.\n",
      "He hath not the come to the world with the worth\n",
      "    The with the worth the strace of the comen and the worth\n",
      "    That he hath the world the too the worth\n",
      "    The with the proce the world to the properself,\n",
      "    The was the good the like the surrest the waster,\n",
      "    And the like the parter the paster the was\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"y part, Iâ€™ll not meddle nor\n",
      "make no fa\"\n",
      "y part, Iâ€™ll not meddle nor\n",
      "make no fallow to the strongeresty to\n",
      "                                                                                                                                                                                                                                                                                                                              Exeunter ROTESS\n",
      "\n",
      "                                     \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"y part, Iâ€™ll not meddle nor\n",
      "make no fa\"\n",
      "y part, Iâ€™ll not meddle nor\n",
      "make no falysins!\n",
      "Shou. Sird this ulpwomf anawhing been ouch an antersure theres?\n",
      "  QUEEN GOUET.\n",
      "No, len; that gaitedely follines, Prontia, would in to true.\n",
      "    leve with the fillrack can your sence, in â€™tige.\n",
      "Pray you it thee, fune\n",
      "Him thke that make sunstamble in oneal the ewarst, Erw.\n",
      "  LUbExe. He Greswortst to the Sighted counter,\n",
      "Agner porputy\n",
      "whad out twere pery-act thysere of them.\n",
      "  CRINCENTO. Mu\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"y part, Iâ€™ll not meddle nor\n",
      "make no fa\"\n",
      "y part, Iâ€™ll not meddle nor\n",
      "make no fathers; do and seemy men ely\n",
      "    Beforalord in the entry,\n",
      "    Jigg't o't our velisectâ€™d?\n",
      "\n",
      "HARLETR. Have we ampy conf'thosl;\n",
      "    Huserely shopen olfore ot mo.\n",
      "\n",
      "FORDOF.\n",
      "Fainâ€™s bull repome of horravite and thank seent that, the waer ro?\n",
      "  TACONT MANCEUND., brote frryge oving; ble weal wench more thy\n",
      "brince sut oh hopold madole's cheors his anbot.\n",
      "\n",
      " ELONGEL.\n",
      "I gool ackence's borstuga a plewites. Yo\n",
      "1122018/1122018 [==============================] - 961s 856us/sample - loss: 1.7187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dd2758ea48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
